{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "greek_news_sites_scrappers_search.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "rZ01caInznb6",
        "kOWKmO1h_tFT",
        "bp4Hp6Q5ftkC",
        "0bJVg-TN_lza"
      ],
      "authorship_tag": "ABX9TyNGp4Vkw/13EwbqUxdpw5PP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tsparaskevas/ML_EDDE2/blob/main/Final%20essay/greek_news_sites_scrappers_search.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Import libraries**"
      ],
      "metadata": {
        "id": "Q0Jd2ifAQ18O"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WhuZHFNzJW7k",
        "outputId": "8868530c-87a2-470d-f625-6b411e2c59d3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at gdrive; to attempt to forcibly remount, call drive.mount(\"gdrive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from datetime import date \n",
        "from datetime import datetime\n",
        "import sys\n",
        "import os\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "\n",
        "import re\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import time\n",
        "\n",
        "from google.colab import drive \n",
        "drive.mount('gdrive') "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####**Define Search pages and Keywords**"
      ],
      "metadata": {
        "id": "OCS5cWpUaGEn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define start and last page numbers"
      ],
      "metadata": {
        "id": "EAhjAZWcrTrZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Start page number\n",
        "#s = 85 #85\n",
        "# Last page number\n",
        "#l = 87 #135"
      ],
      "metadata": {
        "id": "3p1aB7zGrOCh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define search term"
      ],
      "metadata": {
        "id": "NHq5JqS6rqKJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#search_term = \"τροχαίο\""
      ],
      "metadata": {
        "id": "AsE5NGgUrOCf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Newsbomb.gr**"
      ],
      "metadata": {
        "id": "2c6462M5skqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Search pages function**"
      ],
      "metadata": {
        "id": "XnER2HQMQLvb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pjvBU5kHPPdn"
      },
      "outputs": [],
      "source": [
        "def scrape_newsbomb_search_pages(s,l):\n",
        "  base_url = \"https://www.newsbomb.gr/search?q=\" # το βασικό url των σελίδων του search\n",
        "  main_url = base_url + search_term + \"&page=\"\n",
        "  global teaser_soups_list\n",
        "  teaser_soups_list = [] # δημιουργία κενής λίστας για την αποθήκευση όλων των html των σελίδων των κατηγοριών\n",
        "\n",
        "  # Scraping των ζητούμενων σελίδων Latest\n",
        "  for i in range(s, l): \n",
        "    page_url = main_url + str(i) \n",
        "    page = requests.get(page_url) \n",
        "    teaser_soup = BeautifulSoup(page.text, 'html.parser') \n",
        "    print(f\"Now Scraping page: {i}\") \n",
        "    teaser_soups_list.append(teaser_soup) \n",
        "    time.sleep(2)\n",
        "\n",
        "  # Συλλογή των δεδομένων των teaser articles\n",
        "  global teaser_articles_list\n",
        "  teaser_articles_list = [] # δημιουργία κενής λίστας για την αποθήκευση όλων των λεξικών με τα δεδομένα κάθε teaser άρθρου\n",
        "  nbr = 0\n",
        "  for teaser_soup in teaser_soups_list:\n",
        "    teaser_articles = teaser_soup.find('div', {'class': 'list-items viewbox'}).find_all('div', {'class': 'ctype-story'})\n",
        "    for teaser_article in teaser_articles:\n",
        "      teaser_article_dict = {} # δημιουργία κενού λεξικού για την αποθήκευση των δεδομένων κάθε άρθρου\n",
        "      a = teaser_article.find('a', {'class': 'item-link'})\n",
        "      if a:\n",
        "        teaser_article_dict['Url'] = \"https://www.newsbomb.gr\" + a['href'] # url\n",
        "      h3 = teaser_article.find('h3')\n",
        "      if h3:\n",
        "        teaser_article_dict['Title'] = h3.text.replace('\\n', \"\").replace('\\t', \"\") # title\n",
        "      lead = teaser_article.find('div', {'class': 'item-intro'})\n",
        "      if lead:\n",
        "        teaser_article_dict['Lead'] = lead.text.replace('\\n', \"\").replace('\\t', \"\") # lead\n",
        "      category = teaser_article.find('span', {'class': 'item-category'})\n",
        "      if category:\n",
        "        teaser_article_dict['Section'] = category.text # category\n",
        "      teaser_articles_list.append(teaser_article_dict)\n",
        "  # Create dataframe\n",
        "  #teasers_df = pd.DataFrame(teaser_articles_list)\n",
        "  #print(f\"Teasers' dataframe is created. It contains: {len(teaser_articles_list)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Full articles function**"
      ],
      "metadata": {
        "id": "7IKAL1BKtBx8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def scrape_newsbomb_full_articles():\n",
        "  # Scraping όλων των σελίδων των ολόκληρων άρθρων\n",
        "  full_soups_list = [] \n",
        "  nbr = 0 \n",
        "  global teasers_df\n",
        "  for article_url in teasers_df['Url']: \n",
        "    nbr += 1 \n",
        "    full_response = requests.get(article_url) \n",
        "    full_soup = BeautifulSoup(full_response.text, 'html.parser')\n",
        "    full_soups_list.append(full_soup)\n",
        "    time.sleep(2)\n",
        "    print(f'Scraping article {nbr} of {len(teasers_df)}')\n",
        "\n",
        "  # Συλλογή των δεδομένων για τα ολόκληρα άρθρα\n",
        "  global full_articles_list\n",
        "  full_articles_list = []\n",
        "  nbr = 0 # Δημιουργώ αρίθμηση ώστε παραπέμποντας στο index του teasers_df, να φέρνω και το αντίστοιχο url του άρθρου\n",
        "  for soup in full_soups_list:\n",
        "    print(f\"working on article {teasers_df.iloc[[int(nbr)]]['Url'].item()}\")\n",
        "    full_article_dict = {} # δημιουργία κενού λεξικού για την αποθήκευση των δεδομένων κάθε ιστορίας-άρθρου\n",
        "    full_article_dict['Title'] = teasers_df.iloc[[int(nbr)]]['Title'].item() # Title (από το teasers_df)\n",
        "    full_article_dict['Url'] = teasers_df.iloc[[int(nbr)]]['Url'].item() # το Url του άρθρου (από το teasers_df)\n",
        "    full_article_dict['Lead'] = teasers_df.iloc[[int(nbr)]]['Lead'].item() # το Lead του άρθρου (από το teasers_df)\n",
        "    full_article_dict['Section'] = teasers_df.iloc[[int(nbr)]]['Section'].item() # το Url του άρθρου (από το teasers_df)\n",
        "    try:\n",
        "      dtime = soup.find('time') # Time\n",
        "      if dtime:\n",
        "        datetime_str = dtime.text.replace('\\n', '').replace('\\t', '') + \":00\"\n",
        "        full_article_dict['Date'] = datetime.strptime(datetime_str, '%d/%m/%Y %H:%M:%S')\n",
        "    except:\n",
        "      pass\n",
        "    try:\n",
        "      author = soup.find('span', {'class': 'author-name'}) # Author Newsroom\n",
        "      if author:\n",
        "        full_article_dict['Author'] = author.text\n",
        "    except:\n",
        "      pass\n",
        "    try:\n",
        "      author = soup.find('a', {'class': 'author-name'}) # Author\n",
        "      if author:\n",
        "        full_article_dict['Author'] = author.text\n",
        "    except:\n",
        "      pass\n",
        "    try:\n",
        "      paragraphsL = soup.find('div', {'class': 'main-text story-fulltext'}).find_all('p')\n",
        "      if paragraphsL:\n",
        "        paragraphsL = paragraphsL[:-2]\n",
        "        ptextsL = []\n",
        "        for p in paragraphsL: # Body\n",
        "          p_text = p.text\n",
        "          ptextsL.append(p_text)\n",
        "        body = (' ').join(ptextsL)\n",
        "        full_article_dict['Text'] = body\n",
        "    except:\n",
        "      pass\n",
        "    full_articles_list.append(full_article_dict)\n",
        "    nbr += 1\n",
        "  # Create dataframe\n",
        "  #full_articles_df = pd.DataFrame(full_articles_list)\n",
        "  #print(f\"Full articles' dataframe is created. It contains: {len(full_articles_list)}\")"
      ],
      "metadata": {
        "id": "blKestKjo9Oa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Dikaiologitika.gr**"
      ],
      "metadata": {
        "id": "iUrqnp7qRopO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Search pages function**"
      ],
      "metadata": {
        "id": "gWK4e7vvY9FO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def scrape_dikaiologitika_search_pages(s,l):\n",
        "  base_url = \"https://www.dikaiologitika.gr/site/search/\" # το βασικό url των σελίδων του search\n",
        "  main_url = base_url + search_term + \"?searchword=\" + search_term + \"&start=\"\n",
        "  global teaser_soups_list\n",
        "  teaser_soups_list = [] # δημιουργία κενής λίστας για την αποθήκευση όλων των html των σελίδων των κατηγοριών\n",
        "\n",
        "  # Scraping των ζητούμενων σελίδων Latest\n",
        "  for i in range(s, l+1, 20): \n",
        "    page_url = main_url + str(i) \n",
        "    page = requests.get(page_url) \n",
        "    teaser_soup = BeautifulSoup(page.text, 'html.parser') \n",
        "    print(f\"Now Scraping page: {i}\") \n",
        "    teaser_soups_list.append(teaser_soup) \n",
        "    time.sleep(2)\n",
        "\n",
        "  # Συλλογή των δεδομένων των teaser articles\n",
        "  global teaser_articles_list\n",
        "  teaser_articles_list = [] # δημιουργία κενής λίστας για την αποθήκευση όλων των λεξικών με τα δεδομένα κάθε teaser άρθρου\n",
        "  nbr = 0\n",
        "    \n",
        "  # Scraping των ζητούμενων σελίδων Latest\n",
        "  for teaser_soup in teaser_soups_list:\n",
        "    teaser_articles = teaser_soup.find('div', {'class': 'itemListLowerGrid'}).find_all('div', {'class': 'itemContainer'})\n",
        "    for teaser_article in teaser_articles:\n",
        "      teaser_article_dict = {} # δημιουργία κενού λεξικού για την αποθήκευση των δεδομένων κάθε άρθρου\n",
        "      h3_a = teaser_article.find('h3', {'class': 'genericItemTitle'}).find('a')\n",
        "      if h3_a:\n",
        "        teaser_article_dict['Url'] = \"https://www.dikaiologitika.gr\" + h3_a['href'] # url\n",
        "        teaser_article_dict['Title'] = h3_a.text # title\n",
        "      else:\n",
        "        teaser_article_dict['Url'] = \" \"\n",
        "        teaser_article_dict['Title'] = \" \"       \n",
        "      category = teaser_article.find('span', {'class': 'genericItemCategory'})\n",
        "      if category:\n",
        "        teaser_article_dict['Section'] = category.text # section\n",
        "      else:\n",
        "        teaser_article_dict['Section'] = \" \"\n",
        "      dtime = teaser_article.find('span', {'class', 'genericItemDateCreated'}) \n",
        "      if dtime:\n",
        "        datetime_str = dtime.text.replace('- ', '') + \":00\"\n",
        "        teaser_article_dict['Date'] = datetime.strptime(datetime_str, '%d/%m/%Y %H:%M:%S') # date\n",
        "      else:\n",
        "        teaser_article_dict['Date'] = \" \"\n",
        "      teaser_articles_list.append(teaser_article_dict)\n",
        "  print(f'# of dictionaries created: {len(teaser_articles_list)}')\n",
        "\n",
        "  # Create dataframe\n",
        "  #teasers_df = pd.DataFrame(teaser_articles_list)\n",
        "  #print(f\"Teasers' dataframe is created. It contains: {len(teaser_articles_list)}\")"
      ],
      "metadata": {
        "id": "hRtgs8ECRtHW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Full articles function**"
      ],
      "metadata": {
        "id": "apv0wYl9oAu5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def scrape_dikaiologitika_full_articles():\n",
        "  # Scraping όλων των σελίδων των ολόκληρων άρθρων\n",
        "  full_soups_list = [] \n",
        "  nbr = 0 \n",
        "  global teasers_df\n",
        "  for article_url in teasers_df['Url']: \n",
        "    nbr += 1 \n",
        "    full_response = requests.get(article_url) \n",
        "    full_soup = BeautifulSoup(full_response.text, 'html.parser')\n",
        "    full_soups_list.append(full_soup)\n",
        "    time.sleep(2)\n",
        "    print(f'Scraping article {nbr} of {len(teasers_df)}')\n",
        "\n",
        "  # Συλλογή των δεδομένων για τα ολόκληρα άρθρα\n",
        "  global full_articles_list\n",
        "  full_articles_list = []\n",
        "  nbr = 0 # Δημιουργώ αρίθμηση ώστε παραπέμποντας στο index του teasers_df, να φέρνω και το αντίστοιχο url του άρθρου\n",
        "  for soup in full_soups_list:\n",
        "    print(f\"working on article {teasers_df.iloc[[int(nbr)]]['Url'].item()}\")\n",
        "    full_article_dict = {} # δημιουργία κενού λεξικού για την αποθήκευση των δεδομένων κάθε ιστορίας-άρθρου\n",
        "    full_article_dict['Title'] = teasers_df.iloc[[int(nbr)]]['Title'].item() # Title (από το teasers_df)\n",
        "    full_article_dict['Url'] = teasers_df.iloc[[int(nbr)]]['Url'].item() # το Url του άρθρου (από το teasers_df)\n",
        "    try:\n",
        "      lead = soup.find('div', {'class': 'itemIntroText'}) \n",
        "      if lead:\n",
        "        full_article_dict['Lead'] = lead.text.replace('\\n', '').replace('  ', '') # Lead\n",
        "    except:\n",
        "      pass\n",
        "    full_article_dict['Section'] = teasers_df.iloc[[int(nbr)]]['Section'].item() # το Section του άρθρου (από το teasers_df)\n",
        "    full_article_dict['Date'] = teasers_df.iloc[[int(nbr)]]['Date'].item() # το Date του άρθρου (από το teasers_df)\n",
        "    try:\n",
        "      author = soup.find('div', {'class': 'itemAuthor'}).find('a') \n",
        "      if author:\n",
        "        full_article_dict['Author'] = author.text[1:] # Author Newsroom\n",
        "    except:\n",
        "      pass\n",
        "    try:\n",
        "      paragraphsL = soup.find('div', {'class': 'itemFullText'}).find_all('p')\n",
        "      if paragraphsL:\n",
        "        ptextsL = []\n",
        "        for p in paragraphsL: # Body\n",
        "          p_text = p.text.replace('\\xa0', ' ').replace('\\n', '').replace('  ', '')\n",
        "          ptextsL.append(p_text)\n",
        "        body = (' ').join(ptextsL)\n",
        "        full_article_dict['Text'] = body\n",
        "      else:\n",
        "        full_article_dict['Text'] = \" \"        \n",
        "    except:\n",
        "      pass\n",
        "    full_articles_list.append(full_article_dict)\n",
        "    nbr += 1\n",
        "  \n",
        "  # Create dataframe\n",
        "  #full_articles_df = pd.DataFrame(full_articles_list)\n",
        "  #print(f\"Full articles' dataframe is created. It contains: {len(full_articles_list)}\")"
      ],
      "metadata": {
        "id": "AwQ3_BilnABI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Newsit.gr**"
      ],
      "metadata": {
        "id": "rZ01caInznb6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Search pages function**"
      ],
      "metadata": {
        "id": "TUmZ5d_KzrMr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def scrape_newsit_search_pages(s,l):\n",
        "  base_url = \"https://www.newsit.gr/page/\" # το βασικό url των σελίδων του search\n",
        "  global teaser_soups_list\n",
        "  teaser_soups_list = [] # δημιουργία κενής λίστας για την αποθήκευση όλων των html των σελίδων των κατηγοριών\n",
        "\n",
        "  # Scraping των ζητούμενων σελίδων Latest\n",
        "  for i in range(s, l):\n",
        "    page_url = base_url + str(i) + \"/?s=\" + search_term \n",
        "    page = requests.get(page_url) \n",
        "    teaser_soup = BeautifulSoup(page.text, 'html.parser') \n",
        "    print(f\"Now Scraping page: {i}\") \n",
        "    teaser_soups_list.append(teaser_soup) \n",
        "    time.sleep(2)\n",
        "\n",
        "  # Συλλογή των δεδομένων των teaser articles\n",
        "  global teaser_articles_list\n",
        "  teaser_articles_list = [] # δημιουργία κενής λίστας για την αποθήκευση όλων των λεξικών με τα δεδομένα κάθε teaser άρθρου\n",
        "  nbr = 0\n",
        "  for teaser_soup in teaser_soups_list:\n",
        "    teaser_articles = teaser_soup.find('main', {'class': 'site-main'}).find_all('article')\n",
        "    for teaser_article in teaser_articles:\n",
        "      teaser_article_dict = {} # δημιουργία κενού λεξικού για την αποθήκευση των δεδομένων κάθε άρθρου\n",
        "      h2_a = teaser_article.find('h2').find('a')\n",
        "      if h2_a:\n",
        "        teaser_article_dict['Url'] = h2_a['href'] # url\n",
        "        teaser_article_dict['Title'] = h2_a.text # title\n",
        "      dtime = teaser_article.find('time', {'class': 'entry-date published'})\n",
        "      if dtime:\n",
        "        teaser_article_dict['Date'] = dtime['datetime'].replace('T', ' ')[:-6] # date\n",
        "      teaser_articles_list.append(teaser_article_dict)\n",
        "  print(f'# of dictionaries created: {len(teaser_articles_list)}')\n",
        "  \n",
        "  # Create dataframe\n",
        "  #teasers_df = pd.DataFrame(teaser_articles_list)\n",
        "  #print(f\"Teasers' dataframe is created. It contains: {len(teaser_articles_list)}\")"
      ],
      "metadata": {
        "id": "F_cu9mBAzx8X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Full articles function**"
      ],
      "metadata": {
        "id": "aVzG-jzNzzW5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def scrape_newsit_full_articles():\n",
        "  # Scraping όλων των σελίδων των ολόκληρων άρθρων\n",
        "  full_soups_list = [] \n",
        "  nbr = 0 \n",
        "  global teasers_df\n",
        "  for article_url in teasers_df['Url']: \n",
        "    nbr += 1 \n",
        "    full_response = requests.get(article_url) \n",
        "    full_soup = BeautifulSoup(full_response.text, 'html.parser')\n",
        "    full_soups_list.append(full_soup)\n",
        "    time.sleep(2)\n",
        "    print(f'Scraping article {nbr} of {len(teasers_df)}')\n",
        "\n",
        "  # Συλλογή των δεδομένων για τα ολόκληρα άρθρα\n",
        "  global full_articles_list\n",
        "  full_articles_list = []\n",
        "  nbr = 0 # Δημιουργώ αρίθμηση ώστε παραπέμποντας στο index του teasers_df, να φέρνω και το αντίστοιχο url του άρθρου\n",
        "  for soup in full_soups_list:\n",
        "    print(f\"working on article {teasers_df.iloc[[int(nbr)]]['Url'].item()}\")\n",
        "    full_article_dict = {} # δημιουργία κενού λεξικού για την αποθήκευση των δεδομένων κάθε ιστορίας-άρθρου\n",
        "    full_article_dict['Title'] = teasers_df.iloc[[int(nbr)]]['Title'].item() # Title (από το teasers_df)\n",
        "    full_article_dict['Url'] = teasers_df.iloc[[int(nbr)]]['Url'].item() # το Url του άρθρου (από το teasers_df)\n",
        "    try:\n",
        "      lead = soup.find('div', {'class': 'itemIntroText'}) # Lead\n",
        "      if lead:\n",
        "        full_article_dict['Lead'] = lead.text.replace('\\n', '').replace('  ', '')\n",
        "      else:\n",
        "        full_article_dict['Lead'] = \" \"\n",
        "    except:\n",
        "      pass\n",
        "    try:\n",
        "      section = soup.find('p', {'id': 'breadcrumbs'})\n",
        "      if section:\n",
        "        full_article_dict['Section'] = section.find_all('a')[1].text # Section\n",
        "    except:\n",
        "      pass\n",
        "    full_article_dict['Date'] = teasers_df.iloc[[int(nbr)]]['Date'].item() # το Date του άρθρου (από το teasers_df)\n",
        "    try:\n",
        "      author = soup.find('a', {'rel': 'author'}) # Author\n",
        "      if author:\n",
        "        full_article_dict['Author'] = author.text\n",
        "      else:\n",
        "        full_article_dict['Author'] = \" \"\n",
        "    except:\n",
        "      pass\n",
        "    try:\n",
        "      content = soup.find('div', {'class': 'entry-content post-with-no-excerpt'})\n",
        "      if content:\n",
        "        paragraphsL = content.find_all('p')\n",
        "        ptextsL = []\n",
        "        for p in paragraphsL: # Body\n",
        "          p_text = p.text.replace('\\n', '').replace('\\xa0', '').replace('ΔΙΑΦΗΜΙΣΗ', '').replace('  ', '')\n",
        "          ptextsL.append(p_text)\n",
        "        body = (' ').join(ptextsL)\n",
        "        full_article_dict['Text'] = body\n",
        "      else:\n",
        "        content = soup.find('div', {'class': 'inside-article'})\n",
        "        if content:\n",
        "          paragraphsL = content.find_all('p')\n",
        "          ptextsL = []\n",
        "          for p in paragraphsL:\n",
        "            p_text = p.text.replace('…', '').replace(\"ΔΙΑΦΗΜΙΣΗ\", '').replace('ΟΛΕΣ ΟΙ ΕΙΔΗΣΕΙΣ', '')\n",
        "            ptextsL.append(p_text)\n",
        "          body = (' ').join(ptextsL)\n",
        "          full_article_dict['Text'] = body\n",
        "######\n",
        "#    try:\n",
        "#      paragraphsL = soup.find('div', {'class': 'entry-content post-with-no-excerpt'}).find_all('p')\n",
        "#      if paragraphsL:\n",
        "#        ptextsL = []\n",
        "#        for p in paragraphsL: # Body\n",
        "#          p_text = p.text.replace('\\n', '').replace('\\xa0', '').replace('ΔΙΑΦΗΜΙΣΗ', '').replace('  ', '')\n",
        "#          ptextsL.append(p_text)\n",
        "#        body = (' ').join(ptextsL)\n",
        "#        full_article_dict['Text'] = body\n",
        "#####\n",
        "    except:\n",
        "      pass\n",
        "    full_articles_list.append(full_article_dict)\n",
        "    nbr += 1\n",
        "  \n",
        "  # Create dataframe\n",
        "  #full_articles_df = pd.DataFrame(full_articles_list)\n",
        "  #print(f\"Full articles' dataframe is created. It contains: {len(full_articles_list)}\")"
      ],
      "metadata": {
        "id": "OAf5_69Dz23B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Cnn.gr**"
      ],
      "metadata": {
        "id": "kOWKmO1h_tFT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Search pages function**"
      ],
      "metadata": {
        "id": "bG7Cg-JW_tFc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def scrape_cnn_search_pages(s,l):\n",
        "  base_url = \"https://www.cnn.gr/search?q=\" # το βασικό url των σελίδων του search\n",
        "  main_url = base_url + search_term + \"&page=\"\n",
        "  global teaser_soups_list\n",
        "  teaser_soups_list = [] # δημιουργία κενής λίστας για την αποθήκευση όλων των html των σελίδων των κατηγοριών\n",
        "\n",
        "  # Scraping των ζητούμενων σελίδων Latest\n",
        "  for i in range(s, l):\n",
        "    page_url = main_url + str(i) \n",
        "    page = requests.get(page_url) \n",
        "    teaser_soup = BeautifulSoup(page.text, 'html.parser') \n",
        "    print(f\"Now Scraping page: {i}\") \n",
        "    teaser_soups_list.append(teaser_soup) \n",
        "    time.sleep(2)\n",
        "\n",
        "  # Συλλογή των δεδομένων των teaser articles\n",
        "  global teaser_articles_list\n",
        "  teaser_articles_list = [] # δημιουργία κενής λίστας για την αποθήκευση όλων των λεξικών με τα δεδομένα κάθε teaser άρθρου\n",
        "  nbr = 0\n",
        "  for teaser_soup in teaser_soups_list:\n",
        "    teaser_articles = teaser_soup.find('div', {'class': 'list-items flat-cards'}).find_all('div', {'class': 'gtr'})\n",
        "    for teaser_article in teaser_articles:\n",
        "      teaser_article_dict = {} # δημιουργία κενού λεξικού για την αποθήκευση των δεδομένων κάθε άρθρου\n",
        "      a = teaser_article.find('a')\n",
        "      if a:\n",
        "        teaser_article_dict['Url'] = \"https://www.cnn.gr\" + a['href'] # url\n",
        "      h3 = teaser_article.find('h3')\n",
        "      if h3:\n",
        "        teaser_article_dict['Title'] = h3.text.replace('\\n', \"\").replace(' \\t', \"\").replace('\\t', '') # title\n",
        "      teaser_articles_list.append(teaser_article_dict)\n",
        "  print(f'# of dictionaries created: {len(teaser_articles_list)}')\n",
        "\n",
        "  # Create dataframe\n",
        "  #teasers_df = pd.DataFrame(teaser_articles_list)\n",
        "  #print(f\"Teasers' dataframe is created. It contains: {len(teaser_articles_list)}\")"
      ],
      "metadata": {
        "id": "3G_zOqf3JJY1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Full articles function**"
      ],
      "metadata": {
        "id": "iLSXXNmvLB9W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def scrape_cnn_full_articles():\n",
        "  # Scraping όλων των σελίδων των ολόκληρων άρθρων\n",
        "  full_soups_list = [] \n",
        "  nbr = 0 \n",
        "  global teasers_df\n",
        "  for article_url in teasers_df['Url']: \n",
        "    nbr += 1 \n",
        "    full_response = requests.get(article_url) \n",
        "    full_soup = BeautifulSoup(full_response.text, 'html.parser')\n",
        "    full_soups_list.append(full_soup)\n",
        "    time.sleep(2)\n",
        "    print(f'Scraping article {nbr} of {len(teasers_df)}')\n",
        "\n",
        "  # Συλλογή των δεδομένων για τα ολόκληρα άρθρα\n",
        "  global full_articles_list\n",
        "  full_articles_list = []\n",
        "  nbr = 0 # Δημιουργώ αρίθμηση ώστε παραπέμποντας στο index του teasers_df, να φέρνω και το αντίστοιχο url του άρθρου\n",
        "  for soup in full_soups_list:\n",
        "    print(f\"working on article {teasers_df.iloc[[int(nbr)]]['Url'].item()}\")\n",
        "    full_article_dict = {} # δημιουργία κενού λεξικού για την αποθήκευση των δεδομένων κάθε ιστορίας-άρθρου\n",
        "    full_article_dict['Title'] = teasers_df.iloc[[int(nbr)]]['Title'].item() # Title (από το teasers_df)\n",
        "    full_article_dict['Url'] = teasers_df.iloc[[int(nbr)]]['Url'].item() # το Url του άρθρου (από το teasers_df)\n",
        "    try:\n",
        "      lead = soup.find('div', {'class': 'main-intro story-intro'}) # Lead\n",
        "      if lead:\n",
        "        full_article_dict['Lead'] = lead.find('p').text\n",
        "      else:\n",
        "        full_article_dict['Lead'] = \" \"\n",
        "    except:\n",
        "      pass\n",
        "    try:\n",
        "      section = soup.find('a', {'class': 'main-item-category'})\n",
        "      if section:\n",
        "        full_article_dict['Section'] = section.text.replace('\\n', '').replace('\\t', '') # Section\n",
        "    except:\n",
        "      pass\n",
        "    try:\n",
        "      dtime = soup.find('time')\n",
        "      if datetime:\n",
        "        full_article_dict['Date'] = dtime['datetime'].replace('T', ' ')[:-6] # Date\n",
        "    except:\n",
        "      pass\n",
        "    try:\n",
        "      author = soup.find('span', {'class': 'author-name'}) # Author Newsroom\n",
        "      if author:\n",
        "        full_article_dict['Author'] = author.text\n",
        "      else:\n",
        "        full_article_dict['Author'] = \" \"\n",
        "    except:\n",
        "      pass\n",
        "    try:\n",
        "      paragraphsL = soup.find('div', {'class': 'main-text story-fulltext'}).find_all('p')\n",
        "      if paragraphsL:\n",
        "        ptextsL = []\n",
        "        for p in paragraphsL: # Body\n",
        "          p_text = p.text\n",
        "          ptextsL.append(p_text)\n",
        "        body = (' ').join(ptextsL)\n",
        "        full_article_dict['Text'] = body\n",
        "    except:\n",
        "      pass\n",
        "    full_articles_list.append(full_article_dict)\n",
        "    nbr += 1\n",
        "\n",
        "  # Create dataframe\n",
        "  #full_articles_df = pd.DataFrame(full_articles_list)\n",
        "  #print(f\"Full articles' dataframe is created. It contains: {len(full_articles_list)}\")"
      ],
      "metadata": {
        "id": "4HX6usIOLFaM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Lifo.gr**"
      ],
      "metadata": {
        "id": "bp4Hp6Q5ftkC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Search pages function**"
      ],
      "metadata": {
        "id": "uK0Tvs6KftkD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def scrape_lifo_search_pages(s,l):\n",
        "  base_url = \"https://www.lifo.gr/search?keyword=\" # το βασικό url των σελίδων του search\n",
        "  main_url = base_url + search_term + \"&sort_by=field_publication_date&_wrapper_format=html&page=\"\n",
        "  global teaser_soups_list\n",
        "  teaser_soups_list = [] # δημιουργία κενής λίστας για την αποθήκευση όλων των html των σελίδων των κατηγοριών\n",
        "\n",
        "  # Scraping των ζητούμενων σελίδων Latest\n",
        "  for i in range(s, l):\n",
        "    page_url = main_url + str(i)\n",
        "    page = requests.get(page_url) \n",
        "    teaser_soup = BeautifulSoup(page.text, 'html.parser') \n",
        "    print(f\"Now Scraping page: {i}\") \n",
        "    teaser_soups_list.append(teaser_soup) \n",
        "    time.sleep(2)\n",
        "\n",
        "  # Συλλογή των δεδομένων των teaser articles\n",
        "  global teaser_articles_list\n",
        "  teaser_articles_list = [] # δημιουργία κενής λίστας για την αποθήκευση όλων των λεξικών με τα δεδομένα κάθε teaser άρθρου\n",
        "  nbr = 0\n",
        "  for teaser_soup in teaser_soups_list:\n",
        "    teaser_articles = teaser_soup.find('div', {'class': 'container p-0'}).find_all('article')\n",
        "    for teaser_article in teaser_articles:\n",
        "      teaser_article_dict = {} # δημιουργία κενού λεξικού για την αποθήκευση των δεδομένων κάθε άρθρου\n",
        "      h3 = teaser_article.find('h3').find_all('a')[1]\n",
        "      if h3:\n",
        "        teaser_article_dict['Url'] = h3['href'] # url\n",
        "        teaser_article_dict['Title'] = h3.text[1:-1] # title\n",
        "      dtime = teaser_article.find('time')\n",
        "      if dtime:\n",
        "        teaser_article_dict['Date'] = dtime['datetime'].replace('T', ' ')[:-5] # date\n",
        "      lead = teaser_article.find('span', {'class': 'm-0 p-0 fedrabook fs-6-v lh-10-v font-italic d-none d-lg-block darkText'})\n",
        "      if lead:\n",
        "        teaser_article_dict['Lead'] = lead.text[1:-1] # lead\n",
        "      section = teaser_article.find('h3').find('span', {'class': ' grayText slash'})\n",
        "      if section:\n",
        "        teaser_article_dict['Section'] = section.text # section\n",
        "      teaser_articles_list.append(teaser_article_dict)\n",
        "  print(f'# of dictionaries created: {len(teaser_articles_list)}')\n",
        "\n",
        "  # Create dataframe\n",
        "  #teasers_df = pd.DataFrame(teaser_articles_list)\n",
        "  #print(f\"Teasers' dataframe is created. It contains: {len(teaser_articles_list)}\")"
      ],
      "metadata": {
        "id": "cRNypPS_fyI5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Full articles function**"
      ],
      "metadata": {
        "id": "XuM4VL8bfy-6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def scrape_lifo_full_articles():\n",
        "  # Scraping όλων των σελίδων των ολόκληρων άρθρων\n",
        "  full_soups_list = [] \n",
        "  nbr = 0 \n",
        "  global teasers_df\n",
        "  for article_url in teasers_df['Url']: \n",
        "    nbr += 1 \n",
        "    full_response = requests.get(article_url) \n",
        "    full_soup = BeautifulSoup(full_response.text, 'html.parser')\n",
        "    full_soups_list.append(full_soup)\n",
        "    time.sleep(2)\n",
        "    print(f'Scraping article {nbr} of {len(teasers_df)}')\n",
        "\n",
        "  # Συλλογή των δεδομένων για τα ολόκληρα άρθρα\n",
        "  global full_articles_list\n",
        "  full_articles_list = []\n",
        "  nbr = 0 # Δημιουργώ αρίθμηση ώστε παραπέμποντας στο index του teasers_df, να φέρνω και το αντίστοιχο url του άρθρου\n",
        "  for soup in full_soups_list:\n",
        "    print(f\"working on article {teasers_df.iloc[[int(nbr)]]['Url'].item()}\")\n",
        "    full_article_dict = {} # δημιουργία κενού λεξικού για την αποθήκευση των δεδομένων κάθε ιστορίας-άρθρου\n",
        "    full_article_dict['Title'] = teasers_df.iloc[[int(nbr)]]['Title'].item() # Title (από το teasers_df)\n",
        "    full_article_dict['Url'] = teasers_df.iloc[[int(nbr)]]['Url'].item() # το Url του άρθρου (από το teasers_df)\n",
        "    full_article_dict['Lead'] = teasers_df.iloc[[int(nbr)]]['Lead'].item() # το Lead του άρθρου (από το teasers_df)\n",
        "    full_article_dict['Section'] = teasers_df.iloc[[int(nbr)]]['Section'].item() # το Section του άρθρου (από το teasers_df)\n",
        "    full_article_dict['Date'] = teasers_df.iloc[[int(nbr)]]['Date'].item() # το Date του άρθρου (από το teasers_df)\n",
        "    try:\n",
        "      author = soup.find('a', {'class': 'article__author'}).find('span') # Author Newsroom\n",
        "      if author:\n",
        "        full_article_dict['Author'] = author.text\n",
        "      else:\n",
        "        full_article_dict['Author'] = \" \"\n",
        "    except:\n",
        "      pass\n",
        "    try:  \n",
        "      ptL = []\n",
        "      textL = soup.find('div', {'class': 'article__body article__body--mcap'}).find_all('p')   #bodycontent episode-long-desc\n",
        "      if textL:\n",
        "        for p in textL:\n",
        "            if p.find(class_='article__media article__media--caption'):\n",
        "                continue\n",
        "            else:\n",
        "              p = p.text.replace('\\xa0', ' ').replace('  ', '')\n",
        "              ptL.append(p)\n",
        "        body = (' ').join(ptL)\n",
        "        full_article_dict['Text'] = body\n",
        "      else:\n",
        "        textL = soup.find('div', {'class': 'episode-long-desc'}).find_all('p')\n",
        "        if textL:\n",
        "          for p in textL:\n",
        "              if p.find(class_='article__media article__media--caption'):\n",
        "                  continue\n",
        "              else:\n",
        "                p = p.text.replace('\\xa0', ' ').replace('  ', '')\n",
        "                ptL.append(p)\n",
        "          body = (' ').join(ptL)\n",
        "          full_article_dict['Text'] = body\n",
        "        else:\n",
        "          full_article_dict['Text'] = \" \"\n",
        "    except:\n",
        "      pass    \n",
        "    full_articles_list.append(full_article_dict)\n",
        "    nbr += 1\n",
        "  # Create dataframe\n",
        "  #full_articles_df = pd.DataFrame(full_articles_list)\n",
        "  #print(f\"Full articles' dataframe is created. It contains: {len(full_articles_list)}\")"
      ],
      "metadata": {
        "id": "O3HEArlhfzh5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Naftemporiki.gr**"
      ],
      "metadata": {
        "id": "0bJVg-TN_lza"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Define search pages, keywords AND start/end date**"
      ],
      "metadata": {
        "id": "CDX3zWDogrET"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#search_term = \"τροχαίο\""
      ],
      "metadata": {
        "id": "8rZqdd2PKmrD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#if ' ' in search_term:\n",
        "#  termsL = search_term.split()\n",
        "#  search_term = '+'.join(termsL)\n",
        "#else:\n",
        "#  search_term = search_term\n",
        "#print(search_term)"
      ],
      "metadata": {
        "id": "MSC1JdLdM1iI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#start_date = \"2020-01-01\"\n",
        "#end_date = \"2020-12-31\""
      ],
      "metadata": {
        "id": "-2FQ3FoXmINB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#s = 1\n",
        "#l = 2"
      ],
      "metadata": {
        "id": "Vmz-3OulQJcP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Search pages function**"
      ],
      "metadata": {
        "id": "EPH4V5Xc_rrz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def scrape_naftemporiki_search_pages(s,l):\n",
        "  base_url = \"https://www.naftemporiki.gr/search?q=\" # το βασικό url των σελίδων του search\n",
        "  #main_url = base_url + search_term + \"&sort_by=field_publication_date&_wrapper_format=html&page=\"\n",
        "  global teaser_soups_list\n",
        "  teaser_soups_list = [] # δημιουργία κενής λίστας για την αποθήκευση όλων των html των σελίδων των κατηγοριών\n",
        "\n",
        "  # Scraping των ζητούμενων σελίδων Latest\n",
        "  for i in range(s, l):\n",
        "    page_url = base_url + search_term + \"&FQ=pubdate%3a%5b\" + start_date + \"T00%3a00%3a00%2b02%3a00Z+TO+\" + end_date + \"T00%3a00%3a00%2b02%3a00Z%5d&sort=pubdate+desc&page=\" + str(i)\n",
        "  #  page_url = main_url + str(i)\n",
        "    page = requests.get(page_url) \n",
        "    teaser_soup = BeautifulSoup(page.text, 'html.parser') \n",
        "    print(f\"Now Scraping page: {i}\") \n",
        "    teaser_soups_list.append(teaser_soup) \n",
        "    time.sleep(2)\n",
        "\n",
        "  # Συλλογή των δεδομένων των teaser articles\n",
        "  global teaser_articles_list\n",
        "  teaser_articles_list = [] # δημιουργία κενής λίστας για την αποθήκευση όλων των λεξικών με τα δεδομένα κάθε teaser άρθρου\n",
        "  nbr = 0\n",
        "  for teaser_soup in teaser_soups_list:\n",
        "    teaser_articles = teaser_soup.find_all('div', {'class': 'summary'})\n",
        "    for teaser_article in teaser_articles:\n",
        "      teaser_article_dict = {} # δημιουργία κενού λεξικού για την αποθήκευση των δεδομένων κάθε άρθρου\n",
        "      h4_a = teaser_article.find('h4').find('a')\n",
        "      if h4_a:\n",
        "        teaser_article_dict['Url'] = \"https://naftemporiki.gr\" + h4_a['href'] # url\n",
        "        #print(\"https://naftemporiki.gr\" + h4_a['href'])\n",
        "        teaser_article_dict['Title'] = h4_a.text.replace('\\r', '').replace('\\n', '').replace('  ', '') # title\n",
        "      datetime_obj = teaser_article.find('span', {'class': 'topicDate'}) # datetime\n",
        "      if datetime_obj:\n",
        "        datetime_str = datetime_obj.text.replace('\\r', '').replace('\\n', '').replace(',', '').replace('- ', '').replace('  ', '') + \":00\"\n",
        "        datetime_str = re.sub('Δευτέρα |Τρίτη |Τετάρτη |Πέμπτη |Παρασκευή |Σάββατο |Κυριακή ', '', datetime_str)\n",
        "        datetime_str = re.sub('Ιανουαρίου', '01', datetime_str)\n",
        "        datetime_str = re.sub('Φεβρουαρίου', '02', datetime_str)\n",
        "        datetime_str = re.sub('Μαρτίου', '03', datetime_str)\n",
        "        datetime_str = re.sub('Απριλίου', '04', datetime_str)\n",
        "        datetime_str = re.sub('Μαΐου', '05', datetime_str)\n",
        "        datetime_str = re.sub('Ιουνίου', '06', datetime_str)\n",
        "        datetime_str = re.sub('Ιουλίου', '07', datetime_str)\n",
        "        datetime_str = re.sub('Αυγούστου', '08', datetime_str)\n",
        "        datetime_str = re.sub('Σεπτεμβρίου', '09', datetime_str)\n",
        "        datetime_str = re.sub('Οκτωβρίου', '10', datetime_str)\n",
        "        datetime_str = re.sub('Νοεμβρίου', '11', datetime_str)\n",
        "        datetime_str = re.sub('Δεκεμβρίου', '12', datetime_str)\n",
        "        teaser_article_dict['Date'] = datetime.strptime(datetime_str, '%d %m %Y %H:%M:%S') # date\n",
        "      teaser_article_dict['Lead'] = ' ' # No lead\n",
        "      section = teaser_article.find('div', {'class': 'right'})\n",
        "      if section:\n",
        "        teaser_article_dict['Section'] = section.text.replace('\\r', '').replace('\\n', '').replace('  ', '') # section\n",
        "      teaser_articles_list.append(teaser_article_dict)\n",
        "  print(f'# of dictionaries created: {len(teaser_articles_list)}')\n",
        "\n",
        "  # Create dataframe\n",
        "  #teasers_df = pd.DataFrame(teaser_articles_list)\n",
        "  #print(f\"Teasers' dataframe is created. It contains: {len(teaser_articles_list)}\")"
      ],
      "metadata": {
        "id": "gwPinrSv_yRl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Full articles function**"
      ],
      "metadata": {
        "id": "--lzNQv__yw7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def scrape_naftemporiki_full_articles():\n",
        "  # Scraping όλων των σελίδων των ολόκληρων άρθρων\n",
        "  full_soups_list = [] \n",
        "  nbr = 0 \n",
        "  global teasers_df\n",
        "  for article_url in teasers_df['Url']: \n",
        "    nbr += 1 \n",
        "    full_response = requests.get(article_url) \n",
        "    full_soup = BeautifulSoup(full_response.text, 'html.parser')\n",
        "    full_soups_list.append(full_soup)\n",
        "    time.sleep(2)\n",
        "    print(f'Scraping article {nbr} of {len(teasers_df)}')\n",
        "\n",
        "  # Συλλογή των δεδομένων για τα ολόκληρα άρθρα\n",
        "  global full_articles_list\n",
        "  full_articles_list = []\n",
        "  nbr = 0 # Δημιουργώ αρίθμηση ώστε παραπέμποντας στο index του teasers_df, να φέρνω και το αντίστοιχο url του άρθρου\n",
        "  for soup in full_soups_list:\n",
        "    print(f\"working on article {teasers_df.iloc[[int(nbr)]]['Url'].item()}\")\n",
        "    full_article_dict = {} # δημιουργία κενού λεξικού για την αποθήκευση των δεδομένων κάθε ιστορίας-άρθρου\n",
        "    full_article_dict['Title'] = teasers_df.iloc[[int(nbr)]]['Title'].item() # Title (από το teasers_df)\n",
        "    full_article_dict['Url'] = teasers_df.iloc[[int(nbr)]]['Url'].item() # το Url του άρθρου (από το teasers_df)\n",
        "    full_article_dict['Lead'] = teasers_df.iloc[[int(nbr)]]['Lead'].item() # το Lead του άρθρου (από το teasers_df)\n",
        "    full_article_dict['Section'] = teasers_df.iloc[[int(nbr)]]['Section'].item() # το Section του άρθρου (από το teasers_df)\n",
        "    full_article_dict['Date'] = teasers_df.iloc[[int(nbr)]]['Date'].item() # το Date του άρθρου (από το teasers_df)\n",
        "    author = soup.find('a', {'class': 'article__author'})#.find('span') # Author Newsroom\n",
        "    if author:\n",
        "      full_article_dict['Author'] = author.find('span').text\n",
        "    else:\n",
        "      full_article_dict['Author'] = \" \"\n",
        "    try:\n",
        "      paragraphsL = soup.find('span', {'id': 'spBody'}).find_all('p')\n",
        "      if paragraphsL:\n",
        "        ptextsL = []\n",
        "        for p in paragraphsL: # Body\n",
        "          p_text = p.text.replace('\\xa0', ' ').replace('\\n', '').replace('  ', ' ')\n",
        "          ptextsL.append(p_text)\n",
        "        body = (' ').join(ptextsL)\n",
        "        full_article_dict['Text'] = body\n",
        "    except:\n",
        "      pass\n",
        "    full_articles_list.append(full_article_dict)\n",
        "    nbr += 1\n",
        "\n",
        "  # Create dataframe\n",
        "  #full_articles_df = pd.DataFrame(full_articles_list)\n",
        "  #print(f\"Full articles' dataframe is created. It contains: {len(full_articles_list)}\")"
      ],
      "metadata": {
        "id": "OGG9ZljS_3O8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}